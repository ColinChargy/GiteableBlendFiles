#+
# Convertion of .blend files created by Blender <http://www.blender.org/> for easy, semantic and space-saver storage in standard git repository.
#
# This program was made by Colin Chargy <colin@chargy.fr> under the terms of the GNU GPL v3 (or later version) and originally distributed on GitHub.
#
# This program was design based on the blendfile toolset of Lawrence D'Oliveiro <ldo@geek-central.gen.nz>.
#   This toolset can read blend file, *modify the internal information* (this is important since the program you're looking at cannot modify internal data!) and save it back. 
#   You can find it at https://github.com/ldo/blendhack . Thanks you Lawrence for the original work.
#
# For some info of blend file specifications, see
#   "The Mystery of the Blend" <http://www.atmind.nl/blender/mystery_ot_blend.html>
#   The Blender source code, doc/blender_file_format subdirectory
#
# The trouble of git storing big large fat files (such as blend file) is documented and discuss all over the internet.
#   This has nothing to do with binaries files, the issue remains the same with large text files.
#   However, this has to do with the abitlity to compress (via zlib for git) such files (text files are often efficiently compresseable).
#   Some solutions exist but they all need specific versions of git (and I would like to avoid that).
#   To understand why git is so inneficient storing these files, one should understand how git stores files:
#       Every file of every revision of the git history is stored (in the ".git" folder) and indexed by git with its hash (and compressed with zlib).
#       Thus, one file that wouldn't change over multiple revisions (common case, you're not changing every single file of your repo at each commit ;) ),
#       would have the same hash for each revision and thus, would be stored only once. This is perfect.
#       On the other hand, if you would modify one character (or one bit of a binary file) of a specific file at each revision,
#       every considered single file woulf have a different hash and thus would be stored multiple times by git.
#   This behavior would become very problematic for big fat files (proofs are the discussion all over the web about it).
#
# To solve this, this script breaks blend file into block pieces (following blend file internal blocks).
#   That way, git will only stored block that heve changed at every revision. This is true even if the block hasn't the same index in blend file
#   (thus, not the same the same filename) thanks to git hashing system.
#   For example, if you're modifying only the render settings, only the corresponding block(s) of the blend file would be stored in git (the other ones are already here).
# 
# This program allows everyone to use these git concepts with blend file :
#   - Commit message to document your changes (why, how, who, etc).
#   - Diff visualisation : as internal blocks are semantic pieces, having each of them in a diff would allow the user to better undestand the changes.
#       However, this needs pieces to be plain text to allow git to diff them (with its default behavior) and user to be able to read/understand.
#       This is why, whenever possible, this script would store block as plain text.
#
# However, this program shoudn't be used to use these git concepts with blend file :
#   - Commiting only a subpart of a blend file change. This program *cannot* recreate consistent blend files from user change of blocks
#       (because pointer adress would become inconsistent, types and data order/presence, etc).
#       *Only a proper blend file editor should be used to modify blend files* (there is a pretty good one called Blender which is free ;) ).
#   - For the exact same reason, any order operation on the repository content which would end up with a block order and content not generated by Blender
#       shouldn't be used. I'm thinking of any rebase, merge, etc on blend file subpart.
#
# This program is free software: you can redistribute it and/or modify it 
#   the terms of the GNU General Public License as published by the Free Software
#   Foundation, either version 3 of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY
#   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
#   A PARTICULAR PURPOSE. See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
#   along with this program. If not, see <http://www.gnu.org/licenses/>.
#-

import sys
import struct
import gzip
import os
import shutil
import json
from multiprocessing import Pool

blender_sig = b"BLENDER"

KnownCodes = {
	b'DATA' : 'DATA',
	b'GLOB' : 'FileGlobal',
	b'REND' : 'RenderInfos',
	b'TEST' : 'PreviewImg',
	b'WM\x00\x00' : 'wmWindowManager',
	b'SN\x00\x00' : 'bScreen',
	b'SR\x00\x00' : 'bScreeOlderBlender',
	b'MC\x00\x00' : 'MovieClip',
	b'MS\x00\x00' : 'Mask',
	b'SC\x00\x00' : 'Scene',
	b'CU\x00\x00' : 'Curve',
	b'MB\x00\x00' : 'MetaBall',
	b'IM\x00\x00' : 'Image',
	b'CA\x00\x00' : 'Camera',
	b'LA\x00\x00' : 'Lamp',
	b'LT\x00\x00' : 'Lattice',
	b'VF\x00\x00' : 'VectorFont',
	b'KE\x00\x00' : 'ShapeKey',
	b'WO\x00\x00' : 'World',
	b'TX\x00\x00' : 'Text',
	b'SK\x00\x00' : 'Speaker',
	b'SO\x00\x00' : 'Sound',
	b'GR\x00\x00' : 'Group',
	b'AR\x00\x00' : 'bArmature',
	b'AC\x00\x00' : 'bAction',
	b'OB\x00\x00' : 'Object',
	b'MA\x00\x00' : 'Material',
	b'TE\x00\x00' : 'Texture',
	b'ME\x00\x00' : 'Mesh',
	b'PA\x00\x00' : 'ParticleSettings',
	b'NT\x00\x00' : 'NodeTree',
	b'BR\x00\x00' : 'Brush',
	b'PL\x00\x00' : 'Palette',
	b'PC\x00\x00' : 'PaintCurve',
	b'PY\x00\x00' : 'ScriptObsolete',
	b'GD\x00\x00' : 'GreasePencil',
	b'IP\x00\x00' : 'IpoObsolete', # replaced by FCurves in DATA block
	b'LS\x00\x00' : 'FreestyleLineStyle',
	b'LI\x00\x00' : 'Library',
	b'USER' : 'CustomData',
	b'DNA1' : 'StructureDNA',
	b'ENDB' : 'EndOfFile',
}

StrJSON = 'JSON'
StrBinary = 'Binary'

def structread(fromfile, decode_struct) :
	"""reads sufficient bytes from fromfile to be unpacked according to
	decode_struct, and returns the unpacked results."""
	return struct.unpack(decode_struct, fromfile.read(struct.calcsize(decode_struct)))
#end structread

def from_json(json_object):                                   
	if '__class__' in json_object:                               
		if json_object['__class__'] == 'bytes':
			return bytes(json_object['__value__'])            
	return json_object

def to_json(python_object):  
	if isinstance(python_object, bytes):
		res = {'__class__': 'bytes', '__value__': list(python_object)}
		if len(python_object) < 50:
			res['human readeable'] = str(python_object)
		return res
	raise TypeError(repr(python_object) + ' is not JSON serializable')

#	This is a debug class to find out at what point the reconstruction differs from the original file
class WriteCmp():
	def __init__(self, Write, Cmp):
		self.Write = Write
		self.Cmp = Cmp
	
	def write(self, datatowrite):
		readdata = self.Cmp.read(len(datatowrite))
		assert readdata == datatowrite, 'The original file and the reconstructed one differs! Something went wrong. Written data :\n' + str(datatowrite) + '\nRead data :\n' + str(readdata)
		
		self.Write.write(datatowrite)
	
	def __getattr__(self, attr):
		return self.Write.__getattribute__(attr)

def BlockDecode(blockcode, datasize, oldaddr, dna_index, dna_count, content, big_endian, ptrsize):
	if datasize > 100:
		return None	#	Since this decoding is not semantics, JSON here is not very useful and kills perfo on big files
	
	result = {'blockcode' : blockcode, 'datasize' : datasize, 'oldaddr' : oldaddr, 'dna_index' : dna_index, 'dna_count' : dna_count, 'content' : content}
	return result

def BlockEncode(block, big_endian, ptrsize):
	return	struct.pack("%s4s" %({False : "<", True : ">"}[big_endian]), block['blockcode']) + (struct.pack(
					"%sI%sII" % ({False : "<", True : ">"}[big_endian], {4 : "L", 8 : "Q"}[ptrsize]),
					block['datasize'],
					block['oldaddr'],
					block['dna_index'],
					block['dna_count'],
				  ) if block['blockcode'] != b"ENDB" else b'') + block['content']


def WriteBlockToDisk(GeneratedFile, data, blockcode, datasize, oldaddr, dna_index, dna_count, content, block_index):
	EndBlock = blockcode == b"ENDB"
	if not EndBlock:
		decoded_block = BlockDecode(blockcode, datasize, oldaddr, dna_index, dna_count, content, data['big_endian'], data['ptrsize'])
	else:
		decoded_block = None
	recoded_block = (BlockEncode(decoded_block, data['big_endian'], data['ptrsize']) if decoded_block else None)
	SaveAsJSON = bool(decoded_block)
	block_file = '%s.%s.%s' % (str(block_index), (KnownCodes[blockcode] if blockcode in KnownCodes else 'UnkownBlockCode'), ('json' if SaveAsJSON else 'bin'))
	
	with open(os.path.join(GeneratedFile, block_file), 'w' + ('b' if not SaveAsJSON else '')) as f:
		if SaveAsJSON:
			json.dump(decoded_block, f, sort_keys=True, indent='\t', default=to_json)
		else:
			assert decoded_block is None, 'Decode and reencode block does not seems to be the same : \nBlockCode : %s\ndatasize : %s\noldaddr : %s\ndna_index : %s\ndna_count : %s\ncontent : %s\nDecoded block :\n%s\nRecoded block :\n%s' % (str(blockcode), str(datasize), str(oldaddr), str(dna_index), str(dna_count), str(content), str(decoded_block), str(recoded_block))
			
			f.write(struct.pack("%s4s" % ({False : "<", True : ">"}[data['big_endian']]), blockcode))
			if not EndBlock:
				f.write(struct.pack(
					"%sI%sII" % ({False : "<", True : ">"}[data['big_endian']], {4 : "L", 8 : "Q"}[data['ptrsize']]),
					datasize,
					oldaddr,
					dna_index,
					dna_count,
				  ))
			f.write(content)
	return {'filename': block_file, 'datatype': (StrJSON if SaveAsJSON else StrBinary)}

if __name__ == '__main__':
	if len(sys.argv) != 4 or not (sys.argv[1] == '--decodeBlend' or sys.argv[1] == '--encodeBlend'):
		print('USAGE : %s <cmd> <input> <output>\nWith cmd :\n--decodeBlend\n--encodeBlend' % (sys.argv[0]))
		exit(1)
	
	Cmd = sys.argv[1]
	InputFile = sys.argv[2]
	GeneratedFile = sys.argv[3]

	if Cmd == '--decodeBlend':
		origfd = open(InputFile, "rb")
		sig = origfd.read(2)
		origfd.seek(0)
		data = {'compressed' : sig == b"\x1F\x8B"}
		if data['compressed']:
			fd = gzip.GzipFile(mode = "r", fileobj = origfd)
		else :
			fd = origfd
			origfd = None
		
		sig, ptrcode, endiancode, data['version'] = structread(fd, "7s1s1s3s") # note not endian-dependent
		assert sig == blender_sig, "unrecognized file header signature %s" % sig
		data['ptrsize'] = {b"_" : 4, b"-" : 8}[ptrcode]
		data['big_endian'] = bool(endiancode == b"V")
		
		ResByBlockArray = []
		EndBlock = False
		
		with Pool(processes=os.cpu_count() * 2) as pool:
			while not EndBlock:
				datasize = None
				oldaddr = None
				dna_index = None
				dna_count = None
				
				blockcode = b"".join(structread(fd, "%s4c" % ({False : "<", True : ">"}[data['big_endian']])))
				assert blockcode in KnownCodes, "unknown block code %s" % repr(blockcode)
				
				EndBlock = blockcode == b"ENDB"
				if not EndBlock:
					datasize, oldaddr, dna_index, dna_count = structread(fd, "%sI%sII" % ({False : "<", True : ">"}[data['big_endian']], {4 : "L", 8 : "Q"}[data['ptrsize']]))
					content = fd.read(datasize)
				else:
					content = fd.read()
				
				ResByBlockArray.append(pool.apply_async(WriteBlockToDisk, (GeneratedFile, data, blockcode, datasize, oldaddr, dna_index, dna_count, content, len(ResByBlockArray))))
			fd.close()
			pool.close()
			
			data['block_files'] = [res.get() for res in ResByBlockArray]
			
			with open(os.path.join(GeneratedFile, 'BlendFile.json'), "w") as f:
				json.dump(data, f, sort_keys=True, indent='\t', default=to_json)
	else:
		if os.path.exists(GeneratedFile):
			os.remove(GeneratedFile)
		
		with open(os.path.join(InputFile, 'BlendFile.json'), "r") as f:
			data = json.load(f, object_hook=from_json)
		
		outfile = WriteCmp((open, gzip.open)[data['compressed']](GeneratedFile, "wb"), open('/mnt/c/Users/Colin/hubiC/PersoColin/Creations/Video/BlenderAra/Ara.blend', "rb"))
		outfile = (open, gzip.open)[data['compressed']](GeneratedFile, "wb")
		outfile.write(blender_sig + {4 : b"_", 8 : b"-"}[data['ptrsize']] + {False : b"v", True : b"V"}[data['big_endian']] + data['version'] )
		
		for block_file in data['block_files']:
			with open(os.path.join(InputFile, block_file['filename']), 'r' + ('b' if block_file['datatype'] == StrBinary else '')) as f:
				outfile.write((f.read() if block_file['datatype'] == StrBinary else BlockEncode(json.load(f, object_hook=from_json), data['big_endian'], data['ptrsize'])))
		
		outfile.close()
